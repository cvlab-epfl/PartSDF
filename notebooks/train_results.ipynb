{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f693af",
   "metadata": {},
   "source": [
    "Visualize results and trained models.\n",
    "\n",
    "**Used for models trained with `train.py`, e.g., PartSDF.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c562136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e986e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from src import visualization as viz\n",
    "from src import workspace as ws\n",
    "from src.loss import get_loss_recon\n",
    "from src.mesh import create_mesh, create_parts, SdfGridFiller\n",
    "from src.metric import chamfer_distance\n",
    "from src.primitives import slerp_quaternion\n",
    "from src.reconstruct import reconstruct_parts\n",
    "from src.utils import get_color_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f23bd",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d573c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed, get_device\n",
    "\n",
    "seed = 0\n",
    "expdir = \"../experiments/car_partsdf/\"\n",
    "specs = ws.load_specs(expdir)\n",
    "device = specs.get(\"Device\", get_device())\n",
    "\n",
    "print(f\"Experiment {expdir} (on {device})\")\n",
    "#set_seed(seed); print(f\"Seeds initialized to {seed}.\")\n",
    "\n",
    "def load_gt_parts(instance):\n",
    "    parts = []\n",
    "    for i in range(n_parts):\n",
    "        fn = os.path.join(specs[\"DataSource\"], specs[\"Parts\"][\"SamplesDir\"].split(\"/\")[0], \"meshes\", instance, f\"part{i}.obj\")\n",
    "        parts.append(trimesh.load(fn) if os.path.isfile(fn) else trimesh.Trimesh())\n",
    "    return parts\n",
    "\n",
    "clampD = specs[\"ClampingDistance\"]\n",
    "latent_reg = specs[\"LatentRegLambda\"]\n",
    "part_latent_reg = specs[\"Parts\"].get(\"LatentRegLambda\", None)\n",
    "n_parts = specs[\"Parts\"][\"NumParts\"]\n",
    "part_dim = specs[\"Parts\"][\"LatentDim\"]\n",
    "use_poses = specs[\"Parts\"].get(\"UsePoses\", False)\n",
    "use_occ = True if specs.get(\"ImplicitField\", \"SDF\").lower() in [\"occ\", \"occupancy\"] else False\n",
    "\n",
    "logs = ws.load_history(expdir)\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(13,12))\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, name in enumerate(['loss', 'loss_part', 'loss_inter', 'loss_reg_part', 'lr', 'lat_norm']):\n",
    "    if not name in logs:\n",
    "        continue\n",
    "    r, c = i//4, i%4\n",
    "    axs[r,c].axis('on')\n",
    "    axs[r,c].set_title(name)\n",
    "    axs[r,c].plot(range(logs['epoch']), logs[name])\n",
    "    if name+\"-val\" in logs:\n",
    "        axs[r,c].plot(list(range(specs[\"ValidFrequency\"], logs['epoch']+1, specs[\"ValidFrequency\"])), logs[name+\"-val\"])\n",
    "        axs[r,c].legend(['train', 'valid'])\n",
    "    if name == 'lr':\n",
    "        axs[r,c].plot(range(logs['epoch']), logs['lr_lat'])\n",
    "        axs[r,c].legend(['lr', 'lr_lat'])\n",
    "\n",
    "# Evaluation\n",
    "for evaldir in [ws.get_eval_dir(expdir, logs['epoch']), \n",
    "                ws.get_eval_dir(expdir, f\"{logs['epoch']}_parts\")]:\n",
    "    if os.path.isdir(evaldir):\n",
    "        print(f\"\\nLoading evaluation data from {evaldir}\")\n",
    "        metrics = {}\n",
    "        metric_names = ['chamfer', 'iou', 'ic']\n",
    "        for k in metric_names:\n",
    "            filename = os.path.join(evaldir, f\"{k}.json\")\n",
    "            if os.path.isfile(filename):\n",
    "                with open(filename) as f:\n",
    "                    metrics[k] = json.load(f)\n",
    "            else:\n",
    "                metrics[k] = {}\n",
    "        for metric in metric_names:\n",
    "            all_values = list(metrics[metric].values())\n",
    "            all_values = [v for v in all_values if not np.isnan(v)]\n",
    "            print(f\"Average {metric} = {np.mean(all_values) if len(all_values) else np.nan}  ({len(all_values)} shapes)\")\n",
    "            print(f\"Median  {metric} = {np.median(all_values) if len(all_values) else np.nan}  ({len(all_values)} shapes)\")\n",
    "        \n",
    "        # Fig\n",
    "        for _i, k in enumerate(metric_names):\n",
    "            i = 2 * 4 + _i\n",
    "            r, c = i//4, i%4\n",
    "            axs[r,c].axis('on')\n",
    "            axs[r,c].set_title(\"Test \" + k)\n",
    "            axs[r,c].hist(list(metrics[k].values()), bins=20, alpha=0.5)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6c2b5",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = specs[\"SamplesPerScene\"]\n",
    "\n",
    "with open(specs[\"TrainSplit\"]) as f:\n",
    "    instances = json.load(f)\n",
    "if specs.get(\"ValidSplit\", None) is not None:\n",
    "    with open(specs[\"ValidSplit\"]) as f:\n",
    "        instances_v = json.load(f)\n",
    "else:\n",
    "    instances_v = []\n",
    "if specs.get(\"TestSplit\", None) is not None:\n",
    "    with open(specs[\"TestSplit\"]) as f:\n",
    "        instances_t = json.load(f)\n",
    "else:\n",
    "    instances_t = []\n",
    "\n",
    "print(f\"{len(instances)} shapes in train dataset.\")\n",
    "print(f\"{len(instances_v)} shapes in valid dataset.\")\n",
    "print(f\"{len(instances_t)} shapes in test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082042c",
   "metadata": {},
   "source": [
    "## Model and latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_model, get_part_latents, get_part_poses\n",
    "\n",
    "cp_epoch = logs['epoch']\n",
    "latent_dim = specs['LatentDim']\n",
    "model = get_model(specs[\"Network\"], **specs.get(\"NetworkSpecs\", {}), n_parts=n_parts, part_dim=part_dim, use_occ=use_occ).to(device)\n",
    "latents = get_part_latents(len(instances), n_parts, part_dim, specs.get(\"LatentBound\", None), device=device)\n",
    "if use_poses:\n",
    "    poses = get_part_poses(len(instances), n_parts, freeze=True, device=device, fill_nans=True)\n",
    "    ws.load_poses(expdir, poses)\n",
    "\n",
    "try:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    ws.load_latents(expdir, latents, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "except FileNotFoundError as err:\n",
    "    checkpoint = ws.load_checkpoint(expdir)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    latents.load_state_dict(checkpoint['latents_state_dict'])\n",
    "    print(f\"File not found: {err.filename}.\\nLoading checkpoint instead (epoch={checkpoint['epoch']}).\")\n",
    "    del checkpoint\n",
    "\n",
    "# Freeze to avoid possible gradient computations\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "latents.requires_grad_(False)\n",
    "\n",
    "if False:\n",
    "    print(\"Model:\", model)\n",
    "print(f\"Model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "print(f\"{latents.num_embeddings}x{latents.n_parts} latent vectors of size {latents.embedding_dim}.\")\n",
    "if use_poses:\n",
    "    print(f\"Using part poses (pre-computed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6186b95",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.primitives import standardize_quaternion\n",
    "\n",
    "\n",
    "def get_mean_latents_poses(use_poses=use_poses):\n",
    "    \"\"\"Get the average latent and poses from training data.\"\"\"\n",
    "    latent = latents.weight.detach().clone().mean(0, keepdim=True)\n",
    "    # latent = torch.ones(1, n_parts, latent_dim).normal_(0, 0.01).to(device)\n",
    "    pose = poses.weight.detach().clone().mean(0, keepdim=True)\n",
    "    if use_poses:\n",
    "        R = standardize_quaternion(pose[..., :4])\n",
    "        t = pose[..., 4:7]\n",
    "        s = pose[..., 7:10]\n",
    "    else:\n",
    "        R, t, s = None, None\n",
    "    return latent, R, t, s\n",
    "\n",
    "grid_filler = SdfGridFiller(256, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40171278",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, R, t, s = get_mean_latents_poses()\n",
    "# print(\"l\", latent)\n",
    "# print(\"R\", R)\n",
    "# print(\"t\", t)\n",
    "# print(\"s\", s)\n",
    "\n",
    "train_mesh = create_mesh(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=t, s=s)\n",
    "train_parts = create_parts(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=t, s=s)\n",
    "train_parts = trimesh.util.concatenate(get_color_parts(train_parts))\n",
    "viz.plot_render([train_mesh, train_parts], use_texture=[False, True],\n",
    "                titles=[\"Reconstruction\", \"Parts\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False, R=R, t=t, s=s).show()\n",
    "train_parts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802e730",
   "metadata": {},
   "source": [
    "## Train shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances))\n",
    "print(f\"Shape {idx}: {instances[idx]}\")\n",
    "latent = latents(torch.tensor([idx]).to(device))\n",
    "R, t, s = poses(torch.tensor([idx]).to(device)) if use_poses else (None, None, None)\n",
    "\n",
    "train_mesh = create_mesh(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=t, s=s)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx]+\".obj\"))\n",
    "train_parts = create_parts(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=t, s=s)\n",
    "train_parts = trimesh.util.concatenate(get_color_parts(train_parts))\n",
    "viz.plot_render([gt_mesh, train_mesh, train_parts], use_texture=[False, False, True],\n",
    "                titles=[\"GT\", \"Reconstruction\", \"Parts\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False, R=R, t=t, s=s).show()\n",
    "train_parts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b68b16",
   "metadata": {},
   "source": [
    "## Interpolation between train shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b68b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances), size=2).tolist()\n",
    "t = 0.5  # interpolation factor\n",
    "print(f\"Shapes {idx}: {instances[idx[0]]}, {instances[idx[1]]} (t={t:.2f})\")\n",
    "latent = latents(torch.tensor(idx).to(device))\n",
    "latent = (1. - t) * latent[0] + t * latent[1]\n",
    "if use_poses:\n",
    "    R, tr, s = poses(torch.tensor(idx).to(device))\n",
    "    R = slerp_quaternion(R[0], R[1], t)\n",
    "    tr = (1. - t) * tr[0] + t * tr[1]\n",
    "    s = (1. - t) * s[0] + t * s[1]\n",
    "else:\n",
    "    R, tr, s = None, None, None\n",
    "\n",
    "interp_mesh = create_mesh(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=tr, s=s)\n",
    "gt_mesh0 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[0]]+\".obj\"))\n",
    "gt_mesh1 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[1]]+\".obj\"))\n",
    "gt_parts0 = trimesh.util.concatenate(get_color_parts(load_gt_parts(instances[idx[0]])))\n",
    "gt_parts1 = trimesh.util.concatenate(get_color_parts(load_gt_parts(instances[idx[1]])))\n",
    "interp_parts = create_parts(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=R, t=tr, s=s)\n",
    "interp_parts = trimesh.util.concatenate(get_color_parts(interp_parts))\n",
    "viz.plot_render([gt_mesh0, interp_mesh, gt_mesh1, gt_parts0, interp_parts, gt_parts1], \n",
    "                use_texture=[False, False, False, True, True, True], full_backfaces=True,\n",
    "                titles=[\"GT 0\", f\"Reconstruction (t={t:.2f})\", \"GT 1\", \"Parts 1\", f\"Parts (t={t:.2f})\", \"Parts 2\"]).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False, R=R, t=tr, s=s).show()\n",
    "interp_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa495e10",
   "metadata": {},
   "source": [
    "## Test shape\n",
    "First, try to load an already reconstructed shape. If not, will optimize a latent and save the results (without overwriting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "always_reconstruct = False  #Â True to force reconstruction (do not overwrite existing files)\n",
    "idx = np.random.choice(len(instances_t))\n",
    "\n",
    "instance = instances_t[idx]\n",
    "print(f\"Reconstructing test shape {idx} ({instance})\")\n",
    "inter_lambda = specs[\"Parts\"].get(\"IntersectionLambda\", None)\n",
    "inter_temp = specs[\"Parts\"].get(\"IntersectionTemp\", 1.)\n",
    "if inter_lambda is not None:\n",
    "    print(f\"Using intersection loss with lambda={inter_lambda:.2f} and temperature={inter_temp}\")\n",
    "rotations, translations = None, None  # default part poses (None)\n",
    "\n",
    "cp_epoch_str = str(cp_epoch) + \"_parts\"\n",
    "latent_subdir = ws.get_recon_latent_subdir(expdir, cp_epoch_str)\n",
    "mesh_subdir = ws.get_recon_mesh_subdir(expdir, cp_epoch_str)\n",
    "os.makedirs(latent_subdir, exist_ok=True)\n",
    "os.makedirs(mesh_subdir, exist_ok=True)\n",
    "latent_fn = os.path.join(latent_subdir, instance + \".pth\")\n",
    "mesh_fn = os.path.join(mesh_subdir, instance + \".obj\")\n",
    "parts_subdir = ws.get_recon_parts_subdir(expdir, cp_epoch_str)\n",
    "parts_fn = os.path.join(parts_subdir, instance + \".obj\")\n",
    "\n",
    "loss_recon = get_loss_recon(\"L1-Hard\", reduction='none')\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_nans_average(tensor, average):\n",
    "    \"\"\"Replace invalid values (any along dim=-1) by the average.\"\"\"\n",
    "    index = torch.isnan(tensor).any(-1)\n",
    "    tensor[index] = average[index].detach().clone().to(tensor.dtype).to(tensor.device)\n",
    "\n",
    "# Latent: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(latent_fn):\n",
    "    latent = torch.load(latent_fn)\n",
    "    print(f\"Latent norm = {latent.norm():.4f} (existing)\")\n",
    "    if use_poses:\n",
    "        _poses = torch.load(os.path.join(ws.get_recon_poses_subdir(expdir, cp_epoch_str), instance + \".pth\"))\n",
    "        rotations, translations, scales = _poses[..., :4], _poses[..., 4:7], _poses[..., 7:10]\n",
    "else:\n",
    "    npz = np.load(os.path.join(specs[\"DataSource\"], specs[\"SamplesDir\"], instance, specs[\"SamplesFile\"]))\n",
    "    npz_parts = np.load(os.path.join(specs[\"DataSource\"], specs[\"Parts\"][\"SamplesDir\"], instance, specs[\"SamplesFile\"]))\n",
    "    if use_poses:\n",
    "        rotations = torch.tensor(np.load(os.path.join(specs[\"DataSource\"], specs[\"Parts\"][\"ParametersDir\"], instance, \"quaternions.npy\")))\n",
    "        translations = torch.tensor(np.load(os.path.join(specs[\"DataSource\"], specs[\"Parts\"][\"ParametersDir\"], instance, \"translations.npy\")))\n",
    "        scales = torch.tensor(np.load(os.path.join(specs[\"DataSource\"], specs[\"Parts\"][\"ParametersDir\"], instance, \"scales.npy\")))\n",
    "        print(\"Using part poses (GT).\")\n",
    "        # Remove possible Nans (empty parts) \n",
    "        _, _R, _t, _s = get_mean_latents_poses()\n",
    "        fill_nans_average(rotations, _R.squeeze(0))\n",
    "        fill_nans_average(translations, _t.squeeze(0))\n",
    "        fill_nans_average(scales, _s.squeeze(0))\n",
    "    out = reconstruct_parts(model, npz, npz_parts, 400, 8000, 5e-3, loss_recon, specs[\"ReconLossLambda\"], loss_recon,\n",
    "                            specs[\"Parts\"][\"ReconLossLambda\"], part_latent_reg, clampD, None, part_dim, n_parts=n_parts,\n",
    "                            is_part_sdfnet=True, inter_lambda=inter_lambda, inter_temp=inter_temp, \n",
    "                            rotations=rotations, translations=translations, scales=scales, verbose=True, device=device)\n",
    "    if use_poses:\n",
    "        err, latent, rotations, translations, scales = out\n",
    "    else:\n",
    "        err, latent = out\n",
    "    print(f\"Final loss: {err:.6f}, latent norm = {latent.norm():.4f}\")\n",
    "\n",
    "# Mesh: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(mesh_fn):\n",
    "    test_mesh = trimesh.load(mesh_fn)\n",
    "else:\n",
    "    test_mesh = create_mesh(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=rotations, t=translations, s=scales)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instance+\".obj\"))\n",
    "gt_parts = load_gt_parts(instance)\n",
    "gt_parts = trimesh.util.concatenate(get_color_parts(gt_parts))\n",
    "\n",
    "# Chamfer\n",
    "chamfer_samples = 30_000\n",
    "if test_mesh.is_empty:\n",
    "    chamfer_val = float('inf')\n",
    "else:\n",
    "    chamfer_val = chamfer_distance(gt_mesh.sample(chamfer_samples), test_mesh.sample(chamfer_samples))\n",
    "print(f\"Chamfer-distance (x10^4) = {chamfer_val * 1e4:.6f}\")\n",
    "\n",
    "if not always_reconstruct and os.path.isfile(parts_fn):\n",
    "    test_parts = trimesh.load(parts_fn)\n",
    "else:\n",
    "    test_parts = create_parts(model, latent, 128, 32**3, grid_filler=grid_filler, verbose=True, R=rotations, t=translations, s=scales)\n",
    "    test_parts = trimesh.util.concatenate(get_color_parts(test_parts))\n",
    "viz.plot_render([gt_mesh, test_mesh, gt_parts, test_parts], use_texture=[False, False, True, True], max_cols=2,\n",
    "                titles=[\"GT\", \"Reconstruction\", \"GT Parts\", \"Parts\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False, R=rotations, t=translations, s=scales).show()\n",
    "\n",
    "test_parts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
