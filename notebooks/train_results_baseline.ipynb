{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f693af",
   "metadata": {},
   "source": [
    "Visualize results and trained baseline models.\n",
    "\n",
    "**Used for models trained with `train_baseline.py`, e.g., LatentModulated (:= PartSDF-1Part).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c562136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e986e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from src import visualization as viz\n",
    "from src import workspace as ws\n",
    "from src.loss import get_loss_recon\n",
    "from src.mesh import create_mesh, SdfGridFiller\n",
    "from src.metric import chamfer_distance\n",
    "from src.reconstruct import reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f23bd",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d573c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed, get_device\n",
    "\n",
    "seed = 0\n",
    "expdir = \"../experiments/car_baseline/\"\n",
    "specs = ws.load_specs(expdir)\n",
    "device = specs.get(\"Device\", get_device())\n",
    "\n",
    "print(f\"Experiment {expdir} (on {device})\")\n",
    "#set_seed(seed); print(f\"Seeds initialized to {seed}.\")\n",
    "\n",
    "clampD = specs[\"ClampingDistance\"]\n",
    "latent_reg = specs[\"LatentRegLambda\"]\n",
    "\n",
    "logs = ws.load_history(expdir)\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(13,12))\n",
    "for ax in axs.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, name in enumerate(['loss', 'part_loss', 'loss_inter', 'loss_reg', 'lr', 'lat_norm']):\n",
    "    if not name in logs:\n",
    "        continue\n",
    "    r, c = i//4, i%4\n",
    "    axs[r,c].axis('on')\n",
    "    axs[r,c].set_title(name)\n",
    "    axs[r,c].plot(range(logs['epoch']), logs[name])\n",
    "    if name+\"-val\" in logs:\n",
    "        axs[r,c].plot(list(range(specs[\"ValidFrequency\"], logs['epoch']+1, specs[\"ValidFrequency\"])), logs[name+\"-val\"])\n",
    "        axs[r,c].legend(['train', 'valid'])\n",
    "    if name == 'lr':\n",
    "        axs[r,c].plot(range(logs['epoch']), logs['lr_lat'])\n",
    "        axs[r,c].legend(['lr', 'lr_lat'])\n",
    "\n",
    "# Evaluation\n",
    "for evaldir in [ws.get_eval_dir(expdir, logs['epoch']), \n",
    "                ws.get_eval_dir(expdir, f\"{logs['epoch']}_parts\")]:\n",
    "    if os.path.isdir(evaldir):\n",
    "        print(f\"\\nLoading evaluation data from {evaldir}\")\n",
    "        metrics = {}\n",
    "        metric_names = ['chamfer', 'iou', 'ic']\n",
    "        for k in metric_names:\n",
    "            filename = os.path.join(evaldir, f\"{k}.json\")\n",
    "            if os.path.isfile(filename):\n",
    "                with open(filename) as f:\n",
    "                    metrics[k] = json.load(f)\n",
    "            else:\n",
    "                metrics[k] = {}\n",
    "        for metric in metric_names:\n",
    "            all_values = list(metrics[metric].values())\n",
    "            all_values = [v for v in all_values if not np.isnan(v)]\n",
    "            print(f\"Average {metric} = {np.mean(all_values) if len(all_values) else np.nan}  ({len(all_values)} shapes)\")\n",
    "            print(f\"Median  {metric} = {np.median(all_values) if len(all_values) else np.nan}  ({len(all_values)} shapes)\")\n",
    "        \n",
    "        # Fig\n",
    "        for _i, k in enumerate(metric_names):\n",
    "            i = 2 * 4 + _i\n",
    "            r, c = i//4, i%4\n",
    "            axs[r,c].axis('on')\n",
    "            axs[r,c].set_title(\"Test \" + k)\n",
    "            axs[r,c].hist(list(metrics[k].values()), bins=20, alpha=0.5)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6c2b5",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = specs[\"SamplesPerScene\"]\n",
    "\n",
    "with open(specs[\"TrainSplit\"]) as f:\n",
    "    instances = json.load(f)\n",
    "if specs.get(\"ValidSplit\", None) is not None:\n",
    "    with open(specs[\"ValidSplit\"]) as f:\n",
    "        instances_v = json.load(f)\n",
    "else:\n",
    "    instances_v = []\n",
    "if specs.get(\"TestSplit\", None) is not None:\n",
    "    with open(specs[\"TestSplit\"]) as f:\n",
    "        instances_t = json.load(f)\n",
    "else:\n",
    "    instances_t = []\n",
    "\n",
    "print(f\"{len(instances)} shapes in train dataset.\")\n",
    "print(f\"{len(instances_v)} shapes in valid dataset.\")\n",
    "print(f\"{len(instances_t)} shapes in test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082042c",
   "metadata": {},
   "source": [
    "## Model and latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_model, get_latents\n",
    "\n",
    "cp_epoch = logs['epoch']\n",
    "latent_dim = specs['LatentDim']\n",
    "model = get_model(specs[\"Network\"], **specs.get(\"NetworkSpecs\", {}), latent_dim=latent_dim).to(device)\n",
    "latents = get_latents(len(instances), latent_dim, specs.get(\"LatentBound\", None), device=device)\n",
    "\n",
    "try:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    ws.load_latents(expdir, latents, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "except FileNotFoundError as err:\n",
    "    checkpoint = ws.load_checkpoint(expdir)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    latents.load_state_dict(checkpoint['latents_state_dict'])\n",
    "    print(f\"File not found: {err.filename}.\\nLoading checkpoint instead (epoch={checkpoint['epoch']}).\")\n",
    "    del checkpoint\n",
    "\n",
    "# Freeze to avoid possible gradient computations\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "latents.requires_grad_(False)\n",
    "\n",
    "if False:\n",
    "    print(\"Model:\", model)\n",
    "print(f\"Model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "print(f\"{latents.num_embeddings} latent vectors of size {latents.embedding_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6186b95",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_filler = SdfGridFiller(256, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802e730",
   "metadata": {},
   "source": [
    "## Train shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances))\n",
    "print(f\"Shape {idx}: {instances[idx]}\")\n",
    "latent = latents(torch.tensor([idx]).to(device))\n",
    "\n",
    "train_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=grid_filler, verbose=True)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx]+\".obj\"))\n",
    "viz.plot_render([gt_mesh, train_mesh], titles=[\"GT\", \"Reconstruction\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "train_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b68b16",
   "metadata": {},
   "source": [
    "## Interpolation between train shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b68b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances), size=2).tolist()\n",
    "t = 0.5  # interpolation factor\n",
    "print(f\"Shapes {idx}: {instances[idx[0]]}, {instances[idx[1]]} (t={t:.2f})\")\n",
    "latent = latents(torch.tensor(idx).to(device))\n",
    "latent = (1. - t) * latent[0] + t * latent[1]\n",
    "\n",
    "interp_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=grid_filler, verbose=True)\n",
    "gt_mesh0 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[0]]+\".obj\"))\n",
    "gt_mesh1 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[1]]+\".obj\"))\n",
    "viz.plot_render([gt_mesh0, interp_mesh, gt_mesh1],\n",
    "                titles=[\"GT 0\", f\"Reconstruction (t={t:.2f})\", \"GT 1\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "interp_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa495e10",
   "metadata": {},
   "source": [
    "## Test shape\n",
    "First, try to load an already reconstructed shape. If not, will optimize a latent and save the results (without overwriting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "always_reconstruct = False  #Â True to force reconstruction (do not overwrite existing files)\n",
    "idx = np.random.choice(len(instances_t))\n",
    "\n",
    "instance = instances_t[idx]\n",
    "print(f\"Reconstructing test shape {idx} ({instance})\")\n",
    "\n",
    "cp_epoch_str = str(cp_epoch)\n",
    "latent_subdir = ws.get_recon_latent_subdir(expdir, cp_epoch_str)\n",
    "mesh_subdir = ws.get_recon_mesh_subdir(expdir, cp_epoch_str)\n",
    "os.makedirs(latent_subdir, exist_ok=True)\n",
    "os.makedirs(mesh_subdir, exist_ok=True)\n",
    "latent_fn = os.path.join(latent_subdir, instance + \".pth\")\n",
    "mesh_fn = os.path.join(mesh_subdir, instance + \".obj\")\n",
    "\n",
    "loss_recon = get_loss_recon(\"L1-Hard\", reduction='none')\n",
    "\n",
    "# Latent: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(latent_fn):\n",
    "    latent = torch.load(latent_fn)\n",
    "    print(f\"Latent norm = {latent.norm():.4f} (existing)\")\n",
    "else:\n",
    "    npz = np.load(os.path.join(specs[\"DataSource\"], specs[\"SamplesDir\"], instance, specs[\"SamplesFile\"]))\n",
    "    err, latent = reconstruct(model, npz, 400, 8000, 5e-3, loss_recon, latent_reg, clampD, None, latent_dim, \n",
    "                              verbose=True)\n",
    "    print(f\"Final loss: {err:.6f}, latent norm = {latent.norm():.4f}\")\n",
    "    if not os.path.isfile(latent_fn):  # save reconstruction\n",
    "        torch.save(latent, latent_fn)\n",
    "# Mesh: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(mesh_fn):\n",
    "    test_mesh = trimesh.load(mesh_fn)\n",
    "else:\n",
    "    test_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=grid_filler, verbose=True)\n",
    "    if not os.path.isfile(mesh_fn):  # save reconstruction\n",
    "        test_mesh.export(mesh_fn)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instance+\".obj\"))\n",
    "\n",
    "# Chamfer\n",
    "chamfer_samples = 30_000\n",
    "if test_mesh.is_empty:\n",
    "    chamfer_val = float('inf')\n",
    "else:\n",
    "    chamfer_val = chamfer_distance(gt_mesh.sample(chamfer_samples), test_mesh.sample(chamfer_samples))\n",
    "print(f\"Chamfer-distance (x10^4) = {chamfer_val * 1e4:.6f}\")\n",
    "\n",
    "viz.plot_render([gt_mesh, test_mesh], titles=[\"GT\", \"Reconstruction\"], full_backfaces=True).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "\n",
    "test_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
